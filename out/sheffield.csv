"Title","Href","Author","Date","Abstract","Keywords","University Name"
"A model of social responsibility for start-ups: developing a cross-fertilisation of responsible innovation, the lean start-up approach, and the quadruple helix approach.","https://eprints.whiterose.ac.uk/207567/","Ryan, M.; Popa, E.O.; Blok, V.","September 2023","In this paper, we cross-fertilise responsible innovation (RI), the Lean Start-up approach (LSA) and the Quadruple Helix approach (QH) within one model: the social responsibility for start-ups model (SRSM). SRSM aims to instil responsibility into the start-up innovation process by ensuring that the voices of stakeholders from all four helices are taken into account, whilst providing an assessment of current impacts in these areas compared with the ambition of start-ups. This helps to identify room for improvement in order to provide an iterative, lean approach for start-ups to inform the strategy of the start-up cycle, which incorporates the four helices of the QH, and the four process requirements found in RI theory (anticipation, reflexivity, inclusiveness, and responsiveness). This model works as a way to operationalize responsibility in start-ups. This paper fills a gap where there is a lack of suitable approaches for start-ups to follow and implement.","Tech","University of Sheffield"
"Exploring the campaign space of non-party activism in the 2017 and 2019 UK general elections.","https://eprints.whiterose.ac.uk/203046/","Temple, L.; Langer, A.I.","August 2023","In recent years there has been an increase in election-focused activity undertaken by non-party organisations. This activism concerns issues such as political education, cross-party collaboration, voter registration, and voter advice. Using the 2017 and 2019 UK general elections as case studies, we take a strategic action field approach to analyse how this campaign space is developing. We demonstrate the existence of competing logics of activism associated with the fields of social movements, electioneering, and ‘civic tech’. This leads to conflicts related to ethos, time, organisational hybridity, activism, funding and regulation, with such issues frequently shaped by the affordances offered by digital technology. Our findings contribute towards better understandings of how these actors operate in, and attempt to influence, the contemporary electoral landscape.","Tech","University of Sheffield"
"Professionals’ decision-making in recommending communication aids in the UK : competing considerations.","https://eprints.whiterose.ac.uk/149037/","Murray, J.; Lynch, Y.; Meredith, S.","March 2019","Existing research suggests the provision of communication aids for children with complex communication needs can have significant positive impacts on health and quality-of-life. The process of clinical decision-making related to the recommendation of high-tech communication aids is not well documented or evaluated, and research evidence related to the provision of these aids remains limited. This study aimed to understand the factors that specialized AAC professionals in the UK consider when recommending high-tech communication aids. Purposive sampling was used to recruit teams to six focus groups, each of which centred on a team’s recent recommendation process (i.e. a discussion following a real-time assessment session, where the team attempted to arrive at an agreed recommendation for a specific child). Thematic network analysis was used to interpret data from the focus group discussions. Participants identified a wide range of child characteristics, access features, and communication aid attributes in weighing up decisions for individual children. Findings suggest that specialized AAC professionals in the UK prioritize access features over language considerations in their communication aid recommendations. An explanatory model was developed to illustrate the interaction effect that several competing considerations may have on decision-making. Implications for clinical practice and future research are discussed.","Tech","University of Sheffield"
"Decision-making in communication aid recommendations in the UK : cultural and contextual influencers.","https://eprints.whiterose.ac.uk/149036/","Lynch, Y.; Murray, J.; Moulam, L.","March 2019","High-tech communication aids are one form of augmentative and alternative communication (AAC) intervention offered to children following an assessment process to identify the most appropriate system based on their needs. Professional recommendations are likely to include consideration of child characteristics and communication aid attributes. Recommendations may be influenced by contextual factors related to the cultural work practices and service context of professionals involved, as well as by contextual factors from the child’s life including their family environment and wider settings. The aim of this study was to explore the influence of cultural and contextual factors on the real-time decision-making processes of specialized AAC professionals in the UK. A total of six teams were recruited to the study. Each team carried out an assessment appointment related to a communication aid recommendation for a child and family. Following the appointment, each team participated in a focus group examining their decision-making processes during the preceding assessment. Inductive coding was used to analyse the transcribed data, and three organizing themes emerged relating to the global theme of Cultural and Contextual Influencers on communication aid decision-making. An explanatory model was developed to illustrate the funnelling effect that contextual factors may have on decision-making, which can substantially alter the nature and timing of a communication aid recommendation. Implications for clinical practice and future research are discussed.","Tech","University of Sheffield"
"Responsive architecture and the problem of obsolescence.","https://eprints.whiterose.ac.uk/82925/","Meagher, M.","January 2014","Responsive architecture, a design field that has arisen in recent decades at the intersection of architecture and computer science, invokes a material response to digital information and implies the capacity of the building to respond dynamically to changing stimuli. The question I will address in the paper is whether it is possible for the responsive components of architecture to become a poetically expressive part of the building, and if so why this result has so rarely been achieved in contemporary and recent built work. The history of attitudes to- ward obsolescence in buildings is investigated as one explanation for the rarity of examples like the one considered here that successfully overcomes the rapid obsolescence of responsive components and makes these elements an integral part of the work of architecture. In conclusion I identify strategies for the design of responsive components as poetically expressive elements of architecture.","Tech","University of Sheffield"
"The use of hand-sanitiser gel facilitates combined morphological and genetic analysis of shelled pteropods.","https://eprints.whiterose.ac.uk/207573/","Choo, L.Q.; Spagliardi, G.; Peijnenburg, K.T.C.A.","February 2023","There is a lack of standardised imaging methods for marine zooplankton due to the difficulty of manipulating small and often fragile specimens. Yet, standardised 2D photographs and 3D scans provide important morphological information to accompany DNA-barcoded specimens for reference databases such as the Barcode of Life Data System (BOLD). Shelled pteropods are considered as bio-indicators to study impacts of ocean acidification, and thus, it is especially important to obtain high-quality records of their fragile aragonitic shells. We used alcohol-based hand sanitiser gel as a medium for photographing pteropods of the genus Limacina prior to micro-CT scanning and destructive DNA analysis. The high viscosity and transparency of the hand sanitiser enabled easy handling of the specimens so that they could be positioned in a standardised orientation and photographed with a stacking microscope. The high-quality photographs provide a record of morphology and allow for subsequent geometric morphometric analyses. This method did not impact the downstream micro-CT and molecular analyses of the same specimens and resulted in publicly available 2D and 3D digital vouchers as well as ten reference DNA barcodes (partial Cytochrome Oxidase I gene sequences). While alcohol-based hand sanitiser entered our daily lives due to a distressing pandemic, we could make use of it as a cheap and easily available resource to make high quality voucher photographs of shelled pteropods. Digital vouchers serve as a record of their morphology for further taxonomic analyses and facilitate studies assessing shell growth and impacts of ocean acidification.","Marine","University of Sheffield"
"Genome-wide phylogeography reveals cryptic speciation in the circumglobal planktonic calcifier Limacina bulimoides.","https://eprints.whiterose.ac.uk/199844/","Choo, L.Q.; Spagliardi, G.; Malinsky, M.","March 2023","Little is known about when and how planktonic species arise and persist in the open ocean without apparent dispersal barriers. Pteropods are planktonic snails with thin shells susceptible to dissolution that are used as bio-indicators of ocean acidification. However, distinct evolutionary units respond to acidification differently, and defining species boundaries is therefore crucial for predicting the impact of changing ocean conditions. In this global population genomic study of the shelled pteropod Limacina bulimoides, we combined genetic (759,000 single nucleotide polymorphisms) and morphometric data from 161 individuals, revealing three major genetic lineages (FST = 0.29–0.41): an “Atlantic lineage” sampled across the Atlantic, an “Indo-Pacific lineage” sampled in the North Pacific and Indian Ocean, and a “Pacific lineage” sampled in the North and South Pacific. A time-calibrated phylogeny suggests that the lineages diverged about 1 million years ago, with estimated effective population size remaining high (~10 million) throughout Pleistocene glacial cycles. We do not observe any signatures of recent hybridization, even in areas of sympatry in the North Pacific. While the lineages are reproductively isolated, they are morphologically cryptic, with overlapping shell shape and shell colour distributions. Despite showing that the circumglobal L. bulimoides consists of multiple species with smaller ranges than initially thought, we found that these pteropods still possess high levels of genetic variability. Our study adds to the growing evidence that speciation is often overlooked in the open ocean, and suggests the presence of distinct biological species within many other currently defined circumglobal planktonic species.","Marine","University of Sheffield"
"The characterisation of Wickerhamomyces anomalus M15, a highly tolerant yeast for bioethanol production using seaweed derived medium.","https://eprints.whiterose.ac.uk/192808/","Turner, W.; Greetham, D.; Du, C.","September 2022","Advanced generation biofuels have potential for replacing fossil fuels as society moves forward into a net-zero carbon future. Marine biomass is a promising source of fermentable sugars for fermentative bioethanol production; however the medium derived from seaweed hydrolysis contains various inhibitors, such as salts that affected ethanol fermentation efficiency. In this study the stress tolerance of a marine yeast, Wickerhamomyces anomalus M15 was characterised. Specific growth rate analysis results showed that Wickerhamomyces anomalus M15 could tolerate up to 600 g/L glucose, 150 g/L xylose and 250 g/L ethanol, respectively. Using simulated concentrated seaweed hydrolysates, W. anomalus M15’s bioethanol production potential using macroalgae derived feedstocks was assessed, in which 5.8, 45.0, and 19.9 g/L ethanol was produced from brown (Laminaria digitata), green (Ulva linza) and red seaweed (Porphyra umbilicalis) based media. The fermentation of actual Ulva spp. hydrolysate harvested from United Kingdom shores resulted in a relatively low ethanol concentration (15.5 g/L) due to challenges that arose from concentrating the seaweed hydrolysate. However, fed-batch fermentation using simulated concentrated green seaweed hydrolysate achieved a concentration of 73 g/L ethanol in fermentations using both seawater and reverse osmosis water. Further fermentations conducted with an adaptive strain W. anomalus M15-500A showed improved bioethanol production of 92.7 g/L ethanol from 200 g/L glucose and reduced lag time from 93 h to 24 h in fermentation with an initial glucose concentration of 500 g/L. The results indicated that strains W. anomalus M15 and W. anomalus M15-500A have great potential for industrial bioethanol production using marine biomass derived feedstocks. It also suggested that if a concentrated high sugar content seaweed hydrolysate could be obtained, the bioethanol concentration could achieve 90 g/L or above, exceeding the minimum industrial production threshold.","Marine","University of Sheffield"
"Analysis of ocean ontologies in three frameworks: a study of law of the sea discourse.","https://eprints.whiterose.ac.uk/190698/","Burns, V.","August 2022","Legal frameworks have historically used a colonial territorialist approach to governing ocean space. The United Nations Convention on the Law of the Sea (1982) represents a theoretical departure from colonial territorialism. Instead, UNCLOS employs a functionalist logic approach that is based on principles of sovereignty and consent and uses administrative reasoning as a basis for decision-making. This paper investigates what ontological principles are employed in the development of UNCLOS and asks how these are reproduced in other frameworks. I consider whether ontologies can be extrapolated and studied as latent but agential positions in ocean law and governance frameworks and examine how they might be obstructive to the development of effective regional ocean governance. Lastly, I ask whether ontological principles can be reformed, and through what type of interventions this might be achieved. Results show that tenets of colonial territorialism persist in UNCLOS as terrestrialising practices that are reappropriated towards marine communities. Further, that there are fundamental ways in which ontological principles are obstructive to conservation goals in ocean governance frameworks. Lastly, while the structural reproduction of ontological principles between frameworks resists intervention, evidence suggests that interventionist legal mechanisms that displace anthropocentrisms may offer distinct opportunities for reform.","Marine","University of Sheffield"
"Is the use of deep learning an appropriate means to locate debris in the ocean without harming aquatic wildlife?","https://eprints.whiterose.ac.uk/205565/","Moorton, Z.; Kurt, Z.; Woo, W.L.","June 2022","With the global issue of marine debris ever expanding, it is imperative that the technology industry steps in. The aim is to find if deep learning can successfully distinguish between marine life and synthetic debris underwater. This study assesses whether we could safely clean up our oceans with Artificial Intelligence without disrupting the delicate balance of aquatic ecosystems. Our research compares a simple convolutional neural network with a VGG-16 model using an original database of 1644 underwater images and a binary classification to sort synthetic material from aquatic life. Our results show first insights to safely distinguishing between debris and life.","Marine","University of Sheffield"
"Things that cling: marine attachments in Eliot.","https://eprints.whiterose.ac.uk/182124/","Murray, R.","August 2021","T.S. Eliot was captivated by marine life forms, particularly creatures that cling. From the “ragged claws” of ancient crustaceans to the firm foothold of the “delicate algae and the sea anemone,” Eliot’s writing is full of creatures whose instinct is to “ hold on tight” while the sea tosses them about. Eliot’s marine imagery is bound up with his thinking about the enabling possibilities of attachment—an idea that runs counter to his longstanding critical reputation as a poet of detachment. From his early reading of biological texts to his late critical writing, Eliot’s work demonstrates a recurrent interest in attachment that serves to complicate and enrich our understanding of his aesthetics.","Marine","University of Sheffield"
"Ten years of demographic modelling of divergence and speciation in the sea.","https://eprints.whiterose.ac.uk/188672/","De Jode, A.; Le Moan, A.; Johannesson, K.","May 2022","Understanding population divergence that eventually leads to speciation is essential for evolutionary biology. High species diversity in the sea was regarded as a paradox when strict allopatry was considered necessary for most speciation events because geographical barriers seemed largely absent in the sea, and many marine species have high dispersal capacities. Combining genome-wide data with demographic modelling to infer the demographic history of divergence has introduced new ways to address this classical issue. These models assume an ancestral population that splits into two subpopulations diverging according to different scenarios that allow tests for periods of gene flow. Models can also test for heterogeneities in population sizes and migration rates along the genome to account, respectively, for background selection and selection against introgressed ancestry. To investigate how barriers to gene flow arise in the sea, we compiled studies modelling the demographic history of divergence in marine organisms and extracted preferred demographic scenarios together with estimates of demographic parameters. These studies show that geographical barriers to gene flow do exist in the sea but that divergence can also occur without strict isolation. Heterogeneity of gene flow was detected in most population pairs suggesting the predominance of semipermeable barriers during divergence. We found a weak positive relationship between the fraction of the genome experiencing reduced gene flow and levels of genome-wide differentiation. Furthermore, we found that the upper bound of the ‘grey zone of speciation’ for our dataset extended beyond that found before, implying that gene flow between diverging taxa is possible at higher levels of divergence than previously thought. Finally, we list recommendations for further strengthening the use of demographic modelling in speciation research. These include a more balanced representation of taxa, more consistent and comprehensive modelling, clear reporting of results and simulation studies to rule out nonbiological explanations for general results.","Marine","University of Sheffield"
"Progress towards an HF radar wind speed measurement method using machine learning.","https://eprints.whiterose.ac.uk/185959/","Wyatt, L.","April 2022","HF radars are now an important part of operational coastal observing systems where they are used primarily for measuring surface currents. Their use for wave and wind direction measurement has also been demonstrated. These measurements are based on physical models of radar backscatter from the ocean surface described in terms of its ocean wave directional spectrum and the influence thereon of the surface current. Although this spectrum contains information about the local wind that is generating the wind sea part of the spectrum, it also includes spectral components propagating into the local area having been generated by winds away from the area i.e., swell. In addition, the relationship between the local wind sea and wind speed depends on fetch and duration. Thus, finding a physical model to extract wind speed from the radar signal is not straightforward. In this paper, methods that have been proposed to date will be briefly reviewed and an alternative approach is developed using machine learning methods. These have been applied to three different data sets using different radar systems in different locations. The results presented here are encouraging and proposals for further development are outlined.","Marine","University of Sheffield"
"Applying the Hilbert Envelope method to refine the ultrasonic technique for piston ring oil film thickness measurements in a marine diesel engine.","https://eprints.whiterose.ac.uk/187456/","Rooke, J.; Li, X.; Dwyer-Joyce, R.S.","April 2022","The greatest frictional contributor in an internal combustion engine is the contact between the piston ring pack and cylinder liner. Therefore, an improved lubrication regime has the potential to raise engine efficiency while lowering emissions, aiding to meet environmental regulations. Previous ultrasonic measurements of the oil film thickness (OFT) between piston rings and the cylinder liner in a marine engine have been subject to several unexpected trends. This article refines the measurement to identify and remove these factors, the trends were found to have arisen due to the detection of ultrasonic reflections from the piston ring outside of the expected alignment zone. The extent of these undesired reflections is thought to be due to the liner thickness providing a relatively large distance for spreading of the ultrasonic wavefront. The hitherto used Frequency Index method (index of the fast Fourier transform (FFT) at the transducer central frequency) has been compared to the Hilbert Envelope method (index at the peak of the Hilbert Envelope) to define the reflection coefficient. The Hilbert Envelope method was found to minimize the impact from reflections outside of the alignment zone, leading to more consistent and expected trends in the reflection coefficient. The OFT determined using the Hilbert Envelope method was typically 20% greater than that of the Frequency Index method, indicating the previous method provided an underestimation of the film thickness. Using the Hilbert Envelope method, the OFT for the first piston ring at the top dead center (TDC) was found to vary from 8.9 μm at full load to 10.4 μm at 25% loading.","Marine","University of Sheffield"
"Biological trait profiles discriminate between native and non-indigenous marine invertebrates.","https://eprints.whiterose.ac.uk/185538/","Quell, F.; Schratzberger, M.; Beauchard, O.","July 2021","The increasing rate of marine invasions to Western Europe in recent decades highlights the importance of addressing the central questions of invasion biology: what allows an invader to be successful, and which species are likely to become invasive? Consensus is currently lacking regarding the key traits that determine invasiveness in marine species and the extent to which invasive and indigenous species differ in their trait compositions. This limits the ability to predict invasive potential. Here we propose a method based on trait profiles which can be used to predict non-indigenous species likely to cause the greatest impact and native species with a tendency for invasion. We compiled a database of 12 key biological and life history traits of 85 non-indigenous and 302 native marine invertebrate species from Western Europe. Using multivariate methods, we demonstrate that biological traits were able to discriminate between native and non-indigenous species with an accuracy of 78%. The main discriminant traits included body size, lifespan, fecundity, offspring protection, burrowing depth and, to a lesser extent, pelagic stage duration. Analysis revealed that the typical non-indigenous marine invertebrate is a mid-sized, long-lived, highly fecund suspension feeder which either broods its offspring or has a pelagic stage duration of 1–30 days, and is either attached-sessile or burrows to a depth of 5 cm. Biological traits were also able to predict native species classed as “potentially invasive” with an accuracy of 78%. Targeted surveillance and proactive management of invasive species requires accurate predictions of which species are likely to become invasive in the future. Our findings add to the growing evidence that non-indigenous species possess a greater affinity for certain traits. These traits are typically present in the profile of “potentially invasive” native species.","Marine","University of Sheffield"
"Pattern, style and timing of British–Irish Ice Sheet advance and retreat over the last 45 000 years: evidence from NW Scotland and the adjacent continental shelf.","https://eprints.whiterose.ac.uk/174135/","Bradwell, T.; Fabel, D.; Clark, C.D.","February 2021","Predicting the future response of ice sheets to climate warming and rising global sea level is important but difficult. This is especially so when fast‐flowing glaciers or ice streams, buffered by ice shelves, are grounded on beds below sea level. What happens when these ice shelves are removed? And how do the ice stream and the surrounding ice sheet respond to the abruptly altered boundary conditions? To address these questions and others we present new geological, geomorphological, geophysical and geochronological data from the ice‐stream‐dominated NW sector of the last British–Irish Ice Sheet (BIIS). The study area covers around 45 000 km2 of NW Scotland and the surrounding continental shelf. Alongside seabed geomorphological mapping and Quaternary sediment analysis, we use a suite of over 100 new absolute ages (including cosmogenic‐nuclide exposure ages, optically stimulated luminescence ages and radiocarbon dates) collected from onshore and offshore, to build a sector‐wide ice‐sheet reconstruction combining all available evidence with Bayesian chronosequence modelling. Using this information we present a detailed assessment of ice‐sheet advance/retreat history, and the glaciological connections between different areas of the NW BIIS sector, at different times during the last glacial cycle. The results show a highly dynamic, partly marine, partly terrestrial, ice‐sheet sector undergoing large size variations in response to sub‐millennial‐scale climatic (Dansgaard–Oeschger) cycles over the last 45 000 years. Superimposed on these trends we identify internally driven instabilities, operating at higher frequency, conditioned by local topographic factors, tidewater dynamics and glaciological feedbacks during deglaciation. Specifically, our new evidence indicates extensive marine‐terminating ice‐sheet glaciation of the NW BIIS sector during Greenland Stadials 12 to 9 – prior to the main ‘Late Weichselian’ ice‐sheet glaciation. After a period of restricted glaciation, in Greenland Interstadials 8 to 6, we find good evidence for rapid renewed ice‐sheet build‐up in NW Scotland, with the Minch ice‐stream terminus reaching the continental shelf edge in Greenland Stadial 5, perhaps only briefly. Deglaciation of the NW sector took place in numerous stages. Several grounding‐zone wedges and moraines on the mid‐ and inner continental shelf attest to significant stabilizations of the ice‐sheet grounding line, or ice margin, during overall retreat in Greenland Stadials 3 and 2, and to the development of ice shelves. NW Lewis was the first substantial present‐day land area to deglaciate, in the first half of Greenland Stadial 3 at a time of globally reduced sea‐level c. 26 ka bp, followed by Cape Wrath at c. 24 ka bp. The topographic confinement of the Minch straits probably promoted ice‐shelf development in early Greenland Stadial 2, providing the ice stream with additional support and buffering it somewhat from external drivers. However, c. 20–19 ka bp, as the grounding‐line migrated into shoreward deepening water, coinciding with a marked change in marine geology and bed strength, the ice stream became unstable. We find that, once underway, grounding‐line retreat proceeded in an uninterrupted fashion with the rapid loss of fronting ice shelves – first in the west, then the east troughs – before eventual glacier stabilization at fjord mouths in NW Scotland by ~17 ka bp. Around the same time, ~19–17 ka bp, ice‐sheet lobes readvanced into the East Minch – possibly a glaciological response to the marine‐instability‐triggered loss of adjacent ice stream (and/or ice shelf) support in the Minch trough. An independent ice cap on Lewis also experienced margin oscillations during mid‐Greenland Stadial 2, with an ice‐accumulation centre in West Lewis existing into the latter part of Heinrich Stadial 1. Final ice‐sheet deglaciation of NW mainland Scotland was punctuated by at least one other coherent readvance at c. 15.5 ka bp, before significant ice‐mass losses thereafter. At the glacial termination, c. 14.5 ka bp, glaciers fed outwash sediment to now‐abandoned coastal deltas in NW mainland Scotland around the time of global Meltwater Pulse 1A. Overall, this work on the BIIS NW sector reconstructs a highly dynamic ice‐sheet oscillating in extent and volume for much of the last 45 000 years. Periods of expansive ice‐sheet glaciation dominated by ice‐streaming were interspersed with periods of much more restricted ice‐cap or tidewater/fjordic glaciation. Finally, this work indicates that the role of ice streams in ice‐sheet evolution is complex but mechanistically important throughout the lifetime of an ice sheet – with ice streams contributing to the regulation of ice‐sheet health but also to the acceleration of ice‐sheet demise via marine ice‐sheet instabilities.","Marine","University of Sheffield"
"Maximum extent and readvance dynamics of the Irish Sea Ice Stream and Irish Sea Glacier since the Last Glacial Maximum.","https://eprints.whiterose.ac.uk/174565/","Scourse, J.D.; Chiverrell, R.C.; Smedley, R.K.","March 2021","The BRITICE-CHRONO Project has generated a suite of recently published radiocarbon ages from deglacial sequences offshore in the Celtic and Irish seas and terrestrial cosmogenic nuclide and optically stimulated luminescence ages from adjacent onshore sites. All published data are integrated here with new geochronological data from Wales in a revised Bayesian analysis that enables reconstruction of ice retreat dynamics across the basin. Patterns and changes in the pace of deglaciation are conditioned more by topographic constraints and internal ice dynamics than by external controls. The data indicate a major but rapid and very short-lived extensive thin ice advance of the Irish Sea Ice Stream (ISIS) more than 300 km south of St George's Channel to a marine calving margin at the shelf break at 25.5 ka; this may have been preceded by extensive ice accumulation plugging the constriction of St George's Channel. The release event between 25 and 26 ka is interpreted to have stimulated fast ice streaming and diverted ice to the west in the northern Irish Sea into the main axis of the marine ISIS away from terrestrial ice terminating in the English Midlands, a process initiating ice stagnation and the formation of an extensive dead ice landscape in the Midlands.","Marine","University of Sheffield"
"Comparison of ring-liner oil film thickness resulting from different injector designs in a diesel marine engine using an ultrasound measurement method.","https://eprints.whiterose.ac.uk/175864/","Rooke, J.; Li, X.; Brunskill, H.","May 2021","The global drive to combat climate change is a primary driving force towards producing greener and cleaner marine diesel engines to meet emission legislations. The main cause of an engine’s parasitic frictional loss is the interaction between piston rings and the cylinder liner. Therefore, the piston ring lubricating oil film has been the focus of much prior research, chiefly focusing on small-scale automotive engines. This work employs the ultrasonic reflectometry technique to evaluate the oil film formation resulting from different lubricant injector arrangements on a large two-stroke marine diesel engine. A series of piezoelectric transducers close to the top dead center (TDC) have quantified the oil film thickness (OFT) across three engine loading levels and three injector configurations. The injector configurations compare a more traditional pulse-jet (PJ) injector to a needle lift-type (NLT) injector, which reduces the rate of lubricant atomization. The results gathered show that the OFT increases with decreased engine load for all injector systems. The needle lift injector has been shown to increase the minimum OFT for the first ring at the TDC, reducing the likelihood of boundary lubrication for this ring while also reducing the amount of lubricant present in the exhaust manifold.","Marine","University of Sheffield"
"Towards control of autonomous surface vehicles in rough seas.","https://eprints.whiterose.ac.uk/157784/","McCullough, D.R.; Jones, B.; Villarreal, O.J.G.; Findeisen, R.; Hirche, S.; Janschek, K.; Mönnigmann, M.","February 2020","This paper addresses the problem of controlling an Autonomous Surface Vehicle (ASV) in rough sea-states, with a view towards minimising wave-induced forces, whilst maintaining headway. This is a challenging control application since, and as is derived in the paper, the interaction between the vessel and the wave disturbance is nonlinear and coupled. This subsequently motivates the novel application of the Real Time Iteration Scheme (RTI) for Nonlinear Model Predictive Control (NMPC) of the ASV. Analysis of the resulting control signal provides an important insight into the role of the wave encounter frequency. Specifically, by actuating at twice the average wave encounter frequency, the nonlinear controller is able to reduce the wave forces, compared to an open-loop controller that achieves the same average velocity.","Marine","University of Sheffield"
"What’s hot and what’s not: making sense of biodiversity ‘hotspots’.","https://eprints.whiterose.ac.uk/168454/","Thompson, M.S.A.; Couce, E.; Webb, T.J.","October 2020","Conserving biogeographic regions with especially high biodiversity, known as biodiversity ‘hotspots’, is intuitive because finite resources can be focussed towards manageable units. Yet, biodiversity, environmental conditions and their relationship are more complex with multidimensional properties. Assessments which ignore this risk failing to detect change, identify its direction or gauge the scale of appropriate intervention. Conflicting concepts which assume assemblages as either sharply delineated communities or loosely collected species have also hampered progress in the way we assess and conserve biodiversity. We focus on the marine benthos where delineating manageable areas for conservation is an attractive prospect because it holds most marine species and constitutes the largest single ecosystem on earth by area. Using two large UK marine benthic faunal datasets, we present a spatially gridded data sampling design to account for survey effects which would otherwise be the principal drivers of diversity estimates. We then assess γ‐diversity (regional richness) with diversity partitioned between α (local richness) and β (dissimilarity), and their change in relation to covariates to test whether defining and conserving biodiversity hotspots is an effective conservation strategy in light of the prevailing forces structuring those assemblages. α‐, β‐ and γ‐diversity hotspots were largely inconsistent with each metric relating uniquely to the covariates, and loosely collected species generally prevailed with relatively few distinct assemblages. Hotspots could therefore be an unreliable means to direct conservation efforts if based on only a component part of diversity. When assessed alongside environmental gradients, α‐, β‐ and γ‐diversity provide a multidimensional but still intuitive perspective of biodiversity change that can direct conservation towards key drivers and the appropriate scale for intervention. Our study also highlights possible temporal declines in species richness over 30 years and thus the need for future integrated monitoring to reveal the causal drivers of biodiversity change.","Marine","University of Sheffield"
"Marine conservation : towards a multi-layered network approach.","https://eprints.whiterose.ac.uk/168222/","Jacob, U.; Beckerman, A.; Antonijevic, M.","September 2020","Valuing, managing and conserving marine biodiversity and a full range of ecosystem services is at the forefront of research and policy agendas. However, biodiversity is being lost at up to a thousand times the average background rate. Traditional disciplinary and siloed conservation approaches are not able to tackle this massive loss of biodiversity because they generally ignore or overlook the interactive and dynamic nature of ecosystems processes, limiting their predictability. To conserve marine biodiversity, we must assess the interactions and impacts among biodiversity and ecosystem services (BD-ES). The scaling up in complexity from single species to entire communities is necessary, albeit challenging, for a deeper understanding of how ecosystem services relate to biodiversity and the roles species have in ecosystem service provision. These interactions are challenging to map, let alone fully assess, but network and system-based approaches provide a powerful way to progress beyond those limitations. Here, we introduce a conceptual multi-layered network approach to understanding how ecosystem services supported by biodiversity drive the total service provision, how different stressors impact BD-ES and where conservation efforts should be placed to optimize the delivery of ecosystem services and protection of biodiversity.","Marine","University of Sheffield"
"Marine20—The marine radiocarbon age calibration curve (0–55,000 cal BP).","https://eprints.whiterose.ac.uk/188238/","Heaton, T.J.; Köhler, P.; Butzin, M.","August 2020","The concentration of radiocarbon (14C) differs between ocean and atmosphere. Radiocarbon determinations from samples which obtained their 14C in the marine environment therefore need a marine-specific calibration curve and cannot be calibrated directly against the atmospheric-based IntCal20 curve. This paper presents Marine20, an update to the internationally agreed marine radiocarbon age calibration curve that provides a non-polar global-average marine record of radiocarbon from 0–55 cal kBP and serves as a baseline for regional oceanic variation. Marine20 is intended for calibration of marine radiocarbon samples from non-polar regions; it is not suitable for calibration in polar regions where variability in sea ice extent, ocean upwelling and air-sea gas exchange may have caused larger changes to concentrations of marine radiocarbon. The Marine20 curve is based upon 500 simulations with an ocean/atmosphere/biosphere box-model of the global carbon cycle that has been forced by posterior realizations of our Northern Hemispheric atmospheric IntCal20 14C curve and reconstructed changes in CO2 obtained from ice core data. These forcings enable us to incorporate carbon cycle dynamics and temporal changes in the atmospheric 14C level. The box-model simulations of the global-average marine radiocarbon reservoir age are similar to those of a more complex three-dimensional ocean general circulation model. However, simplicity and speed of the box model allow us to use a Monte Carlo approach to rigorously propagate the uncertainty in both the historic concentration of atmospheric 14C and other key parameters of the carbon cycle through to our final Marine20 calibration curve. This robust propagation of uncertainty is fundamental to providing reliable precision for the radiocarbon age calibration of marine based samples. We make a first step towards deconvolving the contributions of different processes to the total uncertainty; discuss the main differences of Marine20 from the previous age calibration curve Marine13; and identify the limitations of our approach together with key areas for further work. The updated values for ΔR, the regional marine radiocarbon reservoir age corrections required to calibrate against Marine20, can be found at the data base http://calib.org/marine/.","Marine","University of Sheffield"
"A short note on marine reservoir age simulations used in IntCal20.","https://eprints.whiterose.ac.uk/188239/","Butzin, M.; Heaton, T.J.; Köhler, P.","February 2020","Beyond ~13.9 cal kBP, the IntCal20 radiocarbon (14C) calibration curve is based upon combining data across a range of different archives including corals and planktic foraminifera. In order to reliably incorporate such marine data into an atmospheric curve, we need to resolve these records into their constituent atmospheric signal and marine reservoir age. We present results of marine reservoir age simulations enabling this resolution, applying the LSG ocean general circulation model forced with various climatic background conditions and with atmospheric radiocarbon changes according to the Hulu Cave speleothem record. Simulating the spatiotemporal evolution of reservoir ages between 54,000 and 10,700 cal BP, we find reservoir ages between 500 and 1400 yr in the low- and mid-latitudes, but also more than 3000 yr in the polar seas. Our results are broadly in agreement with available marine radiocarbon reconstructions, with the caveat that continental margins, marginal seas, or tropical lagoons are not properly resolved in our coarse-resolution model.","Marine","University of Sheffield"
"Phylogeographic history of flat periwinkles, Littorina fabalis and L. obtusata.","https://eprints.whiterose.ac.uk/158470/","Sotelo, G.; Duvetorp, M.; Costa, D.","December 2019","Background The flat periwinkles, Littorina fabalis and L. obtusata, are two sister species widely distributed throughout the Northern Atlantic shores with high potential to inform us about the process of ecological speciation in the intertidal. However, whether gene flow has occurred during their divergence is still a matter of debate. A comprehensive assessment of the genetic diversity of these species is also lacking and their main glacial refugia and dispersal barriers remain largely unknown. In order to fill these gaps, we sequenced two mitochondrial genes and two nuclear fragments to perform a phylogeographic analysis of flat periwinkles across their distribution range.","Marine","University of Sheffield"
"Proteorhodopsin overproduction enhances the long-term viability of Escherichia coli.","https://eprints.whiterose.ac.uk/154954/","Song, Y.; Cartron, M.L.; Jackson, P.J.","October 2019","Genes encoding the photoreactive protein proteorhodopsin (PR) have been found in a wide range of marine bacterial species, reflecting the significant contribution that PR makes to energy flux and carbon cycling in ocean ecosystems. PR can also confer advantages to enhance the ability of marine bacteria to survive periods of starvation. Here, we investigate the effect of heterologously produced PR on the viability of Escherichia coli. Quantitative mass spectrometry shows that E. coli, exogenously supplied with the retinal cofactor, assembles as many as 187,000 holo-PR molecules per cell, accounting for approximately 47% of the membrane area; even cells with no retinal synthesize ∼148,000 apo-PR molecules per cell. We show that populations of E. coli cells containing PR exhibit significantly extended viability over many weeks, and we use single-cell Raman spectroscopy (SCRS) to detect holo-PR in 9-month-old cells. SCRS shows that such cells, even incubated in the dark and therefore with inactive PR, maintain cellular levels of DNA and RNA and avoid deterioration of the cytoplasmic membrane, a likely basis for extended viability. The substantial proportion of the E. coli membrane required to accommodate high levels of PR likely fosters extensive intermolecular contacts, suggested to physically stabilize the cell membrane and impart a long-term benefit manifested as extended viability in the dark. We propose that marine bacteria could benefit similarly from a high PR content, with a stabilized cell membrane extending survival when those bacteria experience periods of severe nutrient or light limitation in the oceans.","Marine","University of Sheffield"
"Ship classification and detection based on CNN using GF-3 SAR images.","https://eprints.whiterose.ac.uk/140801/","Ma, M.; Chen, J.; Liu, W.","December 2018","Ocean surveillance via high-resolution Synthetic Aperture Radar (SAR) imageries has been a hot issue because SAR is able to work in all-day and all-weather conditions. The launch of Chinese Gaofen-3 (GF-3) satellite has provided a large number of SAR imageries, making it possible to marine targets monitoring. However, it is difficult for traditional methods to extract effective features to classify and detect different types of marine targets in SAR images. This paper proposes a convolutional neutral network (CNN) model for marine target classification at patch level and an overall scheme for marine target detection in large-scale SAR images. First, eight types of marine targets in GF-3 SAR images are labelled based on feature analysis, building the datasets for further experiments. As for the classification task at patch level, a novel CNN model with six convolutional layers, three pooling layers, and two fully connected layers has been designed. With respect to the detection part, a Single Shot Multi-box Detector with a multi-resolution input (MR-SSD) is developed, which can extract more features at different resolution versions. In order to detect different targets in large-scale SAR images, a whole workflow including sea-land segmentation, cropping with overlapping, detection with MR-SSD model, coordinates mapping, and predicted boxes consolidation is developed. Experiments based on the GF-3 dataset demonstrate the merits of the proposed methods for marine target classification and detection.","Marine","University of Sheffield"
"Ecosystem-based management of coral reefs under climate change.","https://eprints.whiterose.ac.uk/134278/","Harvey, B.J.; Nash, K.L.; Blanchard, J.L.","March 2018","Coral reefs provide food and livelihoods for hundreds of millions of people as well as harbour some of the highest regions of biodiversity in the ocean. However, overexploitation, land-use change and other local anthropogenic threats to coral reefs have left many degraded. Additionally, coral reefs are faced with the dual emerging threats of ocean warming and acidification due to rising CO2 emissions, with dire predictions that they will not survive the century. This review evaluates the impacts of climate change on coral reef organisms, communities and ecosystems, focusing on the interactions between climate change factors and local anthropogenic stressors. It then explores the shortcomings of existing management and the move towards ecosystembased management and resilience thinking, before highlighting the need for climate change-ready marine protected areas (MPAs), reduction in local anthropogenic stressors, novel approaches such as human-assisted evolution and the importance of sustainable socialecological systems. It concludes that designation of climate changeready MPAs, integrated with other management strategies involving stakeholders and participation at multiple scales such as marine spatial planning, will be required to maximise coral reef resilience under climate change. However, efforts to reduce carbon emissions are critical if the long-term efficacy of local management actions is to be maintained and coral reefs are to survive. ","Marine","University of Sheffield"
"Glaciolacustrine deposits formed in an ice-dammed tributary valley in the south-central Pyrenees: new evidence for late Pleistocene climate.","https://eprints.whiterose.ac.uk/127833/","Sancho, C.; Rhodes, E.J.; Arenas, C.","January 2018","Combined geomorphic features, stratigraphic characteristics and sedimentologic interpretation, coupled with optically stimulated luminescence (OSL) dates, of a glacio-fluvio-lacustrine sequence (Linás de Broto, northern Spain) provide new information to understand the palaeoenvironmental significance of dynamics of glacier systems in the south-central Pyrenees during the Last Glacial Cycle (≈130 ka to 14 ka). The Linás de Broto depositional system consisted of a proglacial lake fed primarily by meltwater streams emanating from the small Sorrosal glacier and dammed by a lateral moraine of the Ara trunk glacier. The resulting glacio-fluvio-lacustrine sequence, around 55 m thick, is divided into five lithological units consisting of braided fluvial (gravel deposits), lake margin (gravel and sand deltaic deposits) and distal lake (silt and clay laminites) facies associations. Evolution of the depositional environment reflects three phases of progradation of a high-energy braided fluvial system separated by two phases of rapid expansion of the lake. Fluvial progradation occurred during short periods of ice melting. Lake expansion concurred with ice-dam growth of the trunk glacier. The first lake expansion occurred over a time range between 55 ± 9 ka and 49 ± 11 ka, and is consistent with the age of the Viu lateral moraine (49 ± 8 ka), which marks the maximum areal extent of the Ara glacier during the Last Glacial Cycle. These dates confirm that the maximum areal extent of the glacier occurred during Marine Isotope Stages 4 and 3 in the south-central Pyrenees, thus before the Last Glacial Maximum. The evolution of the Linás de Broto depositional system during this maximum glacier extent was modulated by climate oscillations in the northern Iberian Peninsula, probably related to latitudinal shifts of the atmospheric circulation in the southern North-Atlantic Ocean, and variations in summer insolation intensity.","Marine","University of Sheffield"
"Late Quaternary coastal evolution and aeolian sedimentation in the tectonically-active southern Atacama Desert, Chile.","https://eprints.whiterose.ac.uk/129211/","Nash, D.J.; Bateman, M.D.; Bullard, J.E.","November 2017","Analyses of aeolianites and associated dune, surficial carbonate and marine terrace sediments from north-central Chile (27° 54′ S) yield a record of environmental change for the coastal southern Atacama Desert spanning at least the last glacial-interglacial cycle. Optically stimulated luminescence dating indicates phases of aeolian dune construction at around 130, 111–98, 77–69 and 41–28 ka. Thin-section and stable carbon and oxygen isotope analyses suggest a predominantly marine sediment source for the three oldest dune phases. Aeolianites appear to have accumulated mainly from tectonically-uplifted interglacial marine sediments that were deflated during windier and/or stormier intervals. Bedding orientations indicate that sand-transporting winds varied in direction from S-ESE during MIS 5e and WNW-ESE during MIS 5c-5a. Winds from the southeast quadrant are unusual today in this region of the Atacama, suggesting either major shifts in atmospheric circulation or topographic airflow modification. Thin-section evidence indicates that the aeolianites were cemented by two phases of vadose carbonate, tentatively linked to wetter periods around 70 and 45 ka. Tectonic uplift in the area has proceeded at an average rate of 305–542 mm kyr− 1. The study illustrates the complexity of understanding onshore-offshore sediment fluxes in the context of Late Quaternary sea-level fluctuations for an area undergoing rapid tectonic uplift.","Marine","University of Sheffield"
"The molecular basis of phosphite and hypophosphite recognition by ABC-transporters.","https://eprints.whiterose.ac.uk/124555/","Bisson, C.; Adams, N.B.P.; Stevenson, B.","August 2017","Inorganic phosphate is the major bioavailable form of the essential nutrient phosphorus. However, the concentration of phosphate in most natural habitats is low enough to limit microbial growth. Under phosphate-depleted conditions some bacteria utilise phosphite and hypophosphite as alternative sources of phosphorus, but the molecular basis of reduced phosphorus acquisition from the environment is not fully understood. Here, we present crystal structures and ligand binding affinities of periplasmic binding proteins from bacterial phosphite and hypophosphite ATP-binding cassette transporters. We reveal that phosphite and hypophosphite specificity results from a combination of steric selection and the presence of a P-H…π interaction between the ligand and a conserved aromatic residue in the ligand-binding pocket. The characterisation of high affinity and specific transporters has implications for the marine phosphorus redox cycle, and might aid the use of phosphite as an alternative phosphorus source in biotechnological, industrial and agricultural applications.","Marine","University of Sheffield"
"Evidence for orbital and North Atlantic climate forcing in alpine Southern California between 125 and 10 ka from multi-proxy analyses of Baldwin Lake.","https://eprints.whiterose.ac.uk/117010/","Glover, K.C.; MacDonald, G.M.; Kirby, M.E.","April 2017","We employed a new, multi-proxy record from Baldwin Lake (∼125–10 ka) to examine drivers of terrestrial Southern California climate over long timescales. Correlated bulk organic and biogenic silica proxy data demonstrated high-amplitude changes from 125 to 71 ka, suggesting that summer insolation directly influenced lake productivity during MIS 5. From 60 to 57 ka, hydrologic state changes and events occurred in California and the U.S. Southwest, though the pattern of response varied geographically. Intermediate, less variable levels of winter and summer insolation followed during MIS 3 (57–29 ka), which likely maintained moist conditions in Southern California that were punctuated with smaller-order, millennial-scale events. These Dansgaard-Oeschger events brought enhanced surface temperatures (SSTs) to the eastern Pacific margin, and aridity to sensitive terrestrial sites in the Southwest and Southern California. Low temperatures and reduced evaporation are widespread during MIS 2, though there is increasing evidence for moisture extremes in Southern California from 29 to 20 ka. Our record shows that both orbital-scale radiative forcing and rapid North Atlantic temperature perturbations were likely influences on Southern California climate prior to the last glacial. However, these forcings produced a hydroclimatic response throughout California and the U.S. Southwest that was geographically complex. This work highlights that it is especially urgent to improve our understanding of the response to rapid climatic change in these regions. Enhanced temperature and aridity are projected for the rest of the 21st century, which will place stress on water resources.","Marine","University of Sheffield"
"Cosmogenic exposure age constraints on deglaciation and flow behaviour of a marine-based ice stream in western Scotland, 21–16 ka.","https://eprints.whiterose.ac.uk/118025/","Small, D.; Benetti, S.; Dove, D.","April 2017","Understanding how marine-based ice streams operated during episodes of deglaciation requires geochronological data that constrain both timing of deglaciation and changes in their flow behaviour, such as that from unconstrained ice streaming to topographically restricted flow. We present seventeen new 10Be exposure ages from glacial boulders and bedrock at sites in western Scotland within the area drained by the Hebrides Ice Stream, a marine-based ice stream that drained a large proportion of the former British-Irish Ice Sheet. Exposure ages from Tiree constrain deglaciation of a topographic high within the central zone of the ice stream, from which convergent flowsets were produced during ice streaming. These ages thus constrain thinning of the Hebrides Ice Stream, which, on the basis of supporting information, we infer to represent cessation of ice streaming at 20.6 ± 1.2 ka, 3–4 ka earlier than previously inferred. A period of more topographically restricted flow produced flow indicators superimposed on those relating to full ice stream conditions, and exposure ages from up-stream of these constrain deglaciation to 17.5 ± 1.0 ka. Complete deglaciation of the marine sector of the Hebrides Ice Stream occurred by 17–16 ka at which time the ice margin was located near the present coastline. Exposure ages from the southernmost Outer Hebrides (Mingulay and Barra) indicate deglaciation at 18.9 ± 1.0 and 17.1 ± 1.0 ka respectively, demonstrating that an independent ice cap persisted on the southern Outer Hebrides for 3–4 ka after initial ice stream deglaciation. This suggests that deglaciation of the Hebrides Ice Stream was focused along major submarine troughs. Collectively, our data constrain initial deglaciation and changes in flow regime of the Hebrides Ice Stream, final deglaciation of its marine sector, and deglaciation of the southern portion of the independent Outer Hebrides Ice Cap, providing chronological constraints on future numerical reconstructions of this key sector of the former British-Irish Ice Sheet.","Marine","University of Sheffield"
"Study of solvent-based carbon capture for cargo ships through process modelling and simulation.","https://eprints.whiterose.ac.uk/114402/","Luo, X.; Wang, M.","March 2017","Controlling anthropogenic CO2 emission is crucial to mitigate global warming. Marine CO2 emissions accounts for around 3% of the total CO2 emission worldwide and grows rapidly with increasing demand for passenger and cargo transport. The International Maritime Organization (IMO) has adopted mandatory measures to reduce greenhouse gases (GHGs) emissions from international shipping. This study aims to explore how to apply solvent-based post-combustion carbon capture (PCC) process to capture CO2 from the energy system in a typical cargo ship and to evaluate the cost degrees of different integration options through simulation-based techno-economic assessments. The selected reference cargo ship has a propulsion system consisting of two four-stroke reciprocating engines at a total power of 17 MW. The study first addressed the challenge on model development of the marine diesel engines and then developed the model of the ship energy system. The limitations of implementing onboard carbon capture were discussed. Two integration options between the ship energy system and the carbon capture process were simulated to analyse the thermal performance of the integrated system and to estimate equipment size of the carbon capture process. It was found that the carbon capture level could only reach 73% when the existing ship energy system is integrated with the PCC process due to limited heat and electricity supply for CCS. The cost of CO2 captured is around 77.50 €/ton CO2. With installation of an additional gas turbine to provide extra energy utilities to the capture plant, the carbon capture level could reach 90% whilst the cost of CO2 captured is around 163.07 €/ton CO2, mainly because of 21.41% more fuel consumption for the additional diesel gas turbine. This is the first systematical study in applying solvent-based carbon capture for ships, which will inspire other researchers in this area.","Marine","University of Sheffield"
"Experimental Studies of Turbulent Intensity around a Tidal Turbine Support Structure.","https://eprints.whiterose.ac.uk/114752/","Walker, S. R.; Cappietti, L.","April 2017","Tidal stream energy is a low-carbon energy source. Tidal stream turbines operate in a turbulent environment, and the effect of the structure between the turbine and seabed on this environment is not fully understood. An experimental study using 1:72 scale models based on a commercial turbine design was carried out to study the support structure influence on turbulent intensity around the turbine blades. The study was conducted using the wave-current tank at the Laboratory of Maritime Engineering (LABIMA), University of Florence. A realistic flow environment (ambient turbulent intensity = 11%) was established. Turbulent intensity was measured upstream and downstream of a turbine mounted on two different support structures (one resembling a commercial design, the other the same with an additional vertical element), in order to quantify any variation in turbulence and performance between the support structures. Turbine drive power was used to calculate power generation. Acoustic Doppler velocimetry (ADV) was used to record and calculate upstream and downstream turbulent intensity. In otherwise identical conditions, performance variation of only 4% was observed between two support structures. Turbulent intensity at 1, 3 and 5 blade diameters, both upstream and downstream, showed variation up to 21% between the two cases. The additional turbulent structures generated by the additional element of the second support structure appears to cause this effect, and the upstream propagation of turbulent intensity is believed to be permitted by surface waves. This result is significant for the prediction of turbine array performance.","Marine","University of Sheffield"
"Quantifying heterogeneous responses of fish community size structure using novel combined statistical techniques.","https://eprints.whiterose.ac.uk/96584/","Marshall, A.M.; Bigg, G.R.; van Leeuwen, S.M.","November 2015","To understand changes in ecosystems, the appropriate scale at which to study them must be determined. Large marine ecosystems (LMEs) cover thousands of square kilometres and are a useful classification scheme for ecosystem monitoring and assessment. However, averaging across LMEs may obscure intricate dynamics within. The purpose of this study is to mathematically determine local and regional patterns of ecological change within an LME using empirical orthogonal functions (EOFs). After using EOFs to define regions with distinct patterns of change, a statistical model originating from control theory is applied (Nonlinear AutoRegressive Moving Average with eXogenous input – NARMAX) to assess potential drivers of change within these regions. We have selected spatial data sets (0.5° latitude × 1°longitude) of fish abundance from North Sea fisheries research surveys (spanning 1980–2008) as well as of temperature, oxygen, net primary production and a fishing pressure proxy, to which we apply the EOF and NARMAX methods. Two regions showed significant changes since 1980: the central North Sea displayed a decrease in community size structure which the NARMAX model suggested was linked to changes in fishing; and the Norwegian trench region displayed an increase in community size structure which, as indicated by NARMAX results, was primarily linked to changes in sea-bottom temperature. These regions were compared to an area of no change along the eastern Scottish coast where the model determined the community size structure was most strongly associated to net primary production. This study highlights the multifaceted effects of environmental change and fishing pressures in different regions of the North Sea. Furthermore, by highlighting this spatial heterogeneity in community size structure change, important local spatial dynamics are often overlooked when the North Sea is considered as a broad-scale, homogeneous ecosystem (as normally is the case within the political Marine Strategy Framework Directive).","Marine","University of Sheffield"
"Direct and indirect effects of climate and fishing on changes in coastal ecosystem services: a historical perspective from the North Sea.","https://eprints.whiterose.ac.uk/95405/","Selim, S.A.; Blanchard, J.L.; Bedford, J.","June 2014","Humanity depends on the marine environment for a range of vital ecosystem services, at global (e.g. climate regulation), regional (e.g. commercial fisheries) and local scales (e.g. coastal defence and recreation). At the same time, marine ecosystems have been exploited for centuries, and many systems today are under stress from multiple sources. Recent studies have shown how both climate change and fishing have caused long-term changes in the marine environment. However, there is still poor understanding of how these changes influence change in coastal ecosystem services. In this paper, an integrated modelling approach is used to assess how the final delivery of marine ecosystem services to coastal communities is influenced by the direct and indirect effects of changes in ecosystem processes brought about by climate and human impacts, using fisheries of the North Sea region as a case study. Partial least squares path analysis is used to explore the relationships between drivers of change, marine ecosystem processes and services (landings). A simple conceptual model with four variables—climate, fishing effort, ecosystem process and ecosystem services—is applied to the English North Sea using historic ecological, climatic and fisheries time series spanning 1924–2010 to identify the multiple pathways that might exist. As expected, direct and indirect links between fishing effort, ecosystem processes and service provision were significant. However, links between climate and ecosystem processes were weak. This paper highlights how path analysis can be used for analysing long-term temporal links between ecosystem processes and services following a simplified pathway.","Marine","University of Sheffield"
"Harnessing the climate mitigation, conservation and poverty alleviation potential of seagrasses : prospects for developing blue carbon initiatives and payment for ecosystem service programmes.","https://eprints.whiterose.ac.uk/165330/","Hejnowicz, A.P.; Kennedy, H.; Rudd, M.A.","May 2015","Seagrass ecosystems provide numerous ecosystem services that support coastal communities around the world. They sustain abundant marine life as well as commercial and artisanal fisheries, and help protect shorelines from coastal erosion. Additionally, seagrass meadows are a globally significant sink for carbon and represent a key ecosystem for combating climate change. However, seagrass habitats are suffering rapid global decline. Despite recognition of the importance of “Blue Carbon,” no functioning seagrass restoration or conservation projects supported by carbon finance currently operate, and the policies and frameworks to achieve this have not been developed. Yet, seagrass ecosystems could play a central role in addressing important international research questions regarding the natural mechanisms through which the ocean and the seabed can mitigate climate change, and how ecosystem structure links to service provision. The relative inattention that seagrass ecosystems have received represents both a serious oversight and a major missed opportunity. In this paper we review the prospects of further inclusion of seagrass ecosystems in climate policy frameworks, with a particular focus on carbon storage and sequestration, as well as the potential for developing payment for ecosystem service (PES) schemes that are complementary to carbon management. Prospects for the inclusion of seagrass Blue Carbon in regulatory compliance markets are currently limited; yet despite the risks the voluntary carbon sector offers the most immediately attractive avenue for the development of carbon credits. Given the array of ecosystem services seagrass ecosystems provide the most viable route to combat climate change, ensure seagrass conservation and improve livelihoods may be to complement any carbon payments with seagrass PES schemes based on the provision of additional ecosystem services.","Marine","University of Sheffield"
"The genetic structure of Nautilus pompilius populations surrounding Australia and the Philippines.","https://eprints.whiterose.ac.uk/86888/","Williams, R.C.; Jackson, B.C.; Duvaux, L.","June 2015","Understanding the distribution of genetic diversity in exploited species is fundamental to successful conservation. Genetic structure and the degree of gene flow among populations must be assessed to design appropriate strategies to prevent the loss of distinct populations. The cephalopod Nautilus pompilius is fished unsustainably in the Philippines for the ornamental shell trade and has limited legislative protection, despite the species' recent dramatic decline in the region. Here, we use 14 microsatellite markers to evaluate the population structure of N. pompilius around Australia and the Philippines. Despite their relative geographical proximity, Great Barrier Reef individuals are genetically isolated from Osprey Reef and Shark Reef in the Coral Sea (FST =0.312, 0.229, respectively). Conversely, despite the larger geographical distances between the Philippines and west Australian reefs, samples display a small degree of genetic structure (FST =0.015). Demographic scenarios modelled using approximate Bayesian computation analysis indicate that this limited divergence is not due to contemporary gene flow between the Philippines and west Australia. Instead, present-day genetic similarity can be explained by very limited genetic drift that has occurred due to large average effective population sizes that persisted at both locations following their separation. The lack of connectivity among populations suggests that immigrants from west Australia would not facilitate natural recolonization if Philippine populations were fished to extinction. These data help to rectify the paucity of information on the species' biology currently inhibiting their conservation classification. Understanding population structure can allow us to facilitate sustainable harvesting, thereby preserving the diversity of genetically distinct stocks. This article is protected by copyright. All rights reserved.","Marine","University of Sheffield"
"Glacial geomorphology of Marguerite Bay Palaeo-Ice stream, western Antarctic Peninsula.","https://eprints.whiterose.ac.uk/78545/","Livingstone, SJ; Cofaigh, CO; Stokes, CR","October 2013","None","Marine","University of Sheffield"
"Potential consequences of climate change for primary production and fish production in large marine ecosystems.","https://eprints.whiterose.ac.uk/113851/","Blanchard, J. L.; Jennings, S.; Holmes, R.","September 2012","Existing methods to predict the effects of climate change on the biomass and production of marine communities are predicated on modelling the interactions and dynamics of individual species, a very challenging approach when interactions and distributions are changing and little is known about the ecological mechanisms driving the responses of many species. An informative parallel approach is to develop size-based methods. These capture the properties of food webs that describe energy flux and production at a particular size, independent of species’ ecology. We couple a physical–biogeochemical model with a dynamic, size-based food web model to predict the future effects of climate change on fish biomass and production in 11 large regional shelf seas, with and without fishing effects. Changes in potential fish production are shown to most strongly mirror changes in phytoplankton production. We project declines of 30–60% in potential fish production across some important areas of tropical shelf and upwelling seas, most notably in the eastern Indo-Pacific, the northern Humboldt and the North Canary Current. Conversely, in some areas of the high latitude shelf seas, the production of pelagic predators was projected to increase by 28–89%.","Marine","University of Sheffield"
"The geochemistry of environmentally important trace elements in UK coals, with special reference to the Parkgate coal in the Yorkshire-Nottinghamshire Coalfield, UK.","https://eprints.whiterose.ac.uk/10358/","Spears, D.A.; Tewalt, S.J.","December 2009","The Parkgate coal of Langsettian age in the Yorkshire-Nottinghamshire coalfield is typical of many coals in the UK in that it has a high sulphur (S) content. Detailed information on the distribution of the forms of S, both laterally and vertically through the seam, was known from previous investigations. In the present work, 38 interval samples from five measured sections of the coal were comprehensively analysed for major, minor and trace elements and the significance of the relationships established using both raw and centered log transformed data. The major elements are used to quantify the variations in the inorganic and organic coal components and determine the trace element associations. Pyrite contains nearly all of the Hg, As, Se, Tl and Pb and is also the major source of the Mo, Ni, Cd and Sb. The clays contain the following elements in decreasing order of association: Rb, Cs, Li, Ga, U, Cr, V, Sc, Y, Bi, Cu, Nb, Sn, Te and Th. Nearly all of the Rb is present in the clay fraction, whereas for elements such as V, Cu and U, a significant amount is thought to be present in the organic matter, based on the K vs trace element regression equations. Only Ge, and possibly Be, would appear to have a dominant organic source. The trace element concentrations are calculated for pyrite, the clay fraction and organic matter. For pyrite it is noted that concentrations agree with published data from the Yorkshire-Nottinghamshire coalfield and also that Tl concentrations (median of 0.33 ppm) in the pyrite are greater than either Hg or Cd. Unlike these elements, Tl has attracted less attention and possibly more information is needed on its anthropogenic distribution and impacts on man and the environment. A seawater source is thought to be responsible for the high concentrations of S, Cl and the non-detrital trace elements in the Parkgate coal. Indicative of the seawater control is the Th/U ratio, which expresses the detrital to non-detrital element contributions. Using other elements, similar ratios can be calculated, which in combination offer greater interpretative value. (C) 2009 Elsevier B.V. All rights reserved.","Marine","University of Sheffield"
"Biodiversity research sets sail: showcasing the diversity of marine life.","https://eprints.whiterose.ac.uk/97039/","Webb, T.J.","December 2008","The World Congress on Marine Biodiversity was held in the City of Arts and Sciences, Valencia, from 10 to 15 November 2008, showcasing research on all aspects of marine biodiversity from basic taxonomic exploration to innovative conservation strategies and methods to integrate research into environmental policy.","Marine","University of Sheffield"
"Wave and tidal power measurement using HF radar.","https://eprints.whiterose.ac.uk/10589/","Wyatt, L.R.","January 2007","HF radar systems have been used extensively worldwide to provide surface current measurements for scientific, vessel traffic management, environmental, offshore engineering, search and rescue and other applications. Wave measurement with such systems is beginning to be used in operational systems. Many validations have been carried out demonstrating the accuracy of the parameters measured. This paper addresses the potential application of these systems within the marine renewables industry. Wave and tidal power measurements are presented.","Marine","University of Sheffield"
"Novel ameloblastin variants, contrasting amelogenesis imperfecta phenotypes.","https://eprints.whiterose.ac.uk/207087/","Hany, U.; Watson, C.M.; Liu, L.","December 2023","Amelogenesis imperfecta (AI) comprises a group of rare, inherited disorders with abnormal enamel formation. Ameloblastin (AMBN), the second most abundant enamel matrix protein (EMP), plays a critical role in amelogenesis. Pathogenic biallelic loss-of-function AMBN variants are known to cause recessive hypoplastic AI. A report of a family with dominant hypoplastic AI attributed to AMBN missense change p.Pro357Ser, together with data from animal models, suggests that the consequences of AMBN variants in human AI remain incompletely characterized. Here we describe 5 new pathogenic AMBN variants in 11 individuals with AI. These fall within 3 groups by phenotype. Group 1, consisting of 6 families biallelic for combinations of 4 different variants, have yellow hypoplastic AI with poor-quality enamel, consistent with previous reports. Group 2, with 2 families, appears monoallelic for a variant shared with group 1 and has hypomaturation AI of near-normal enamel volume with pitting. Group 3 includes 3 families, all monoallelic for a fifth variant, which are affected by white hypoplastic AI with a thin intact enamel layer. Three variants, c.209C>G; p.(Ser70*) (groups 1 and 2), c.295T>C; p.(Tyr99His) (group 1), and c.76G>A; p.(Ala26Thr) (group 3) were identified in multiple families. Long-read AMBN locus sequencing revealed these variants are on the same conserved haplotype, implying they originate from a common ancestor. Data presented therefore provide further support for possible dominant as well as recessive inheritance for AMBN-related AI and for multiple contrasting phenotypes. In conclusion, our findings suggest pathogenic AMBN variants have a more complex impact on human AI than previously reported.","AI","University of Sheffield"
"Accelerating AI adoption with responsible AI signals and employee engagement mechanisms in health care.","https://eprints.whiterose.ac.uk/205983/","Wang, W.; Chen, L.; Xiong, M.","June 2021","Artificial Intelligence (AI) technology is transforming the healthcare sector. However, despite this, the associated ethical implications remain open to debate. This research investigates how signals of AI responsibility impact healthcare practitioners’ attitudes toward AI, satisfaction with AI, AI usage intentions, including the underlying mechanisms. Our research outlines autonomy, beneficence, explainability, justice, and non-maleficence as the five key signals of AI responsibility for healthcare practitioners. The findings reveal that these five signals significantly increase healthcare practitioners’ engagement, which subsequently leads to more favourable attitudes, greater satisfaction, and higher usage intentions with AI technology. Moreover, ‘techno-overload’ as a primary ‘techno-stressor’ moderates the mediating effect of engagement on the relationship between AI justice and behavioural and attitudinal outcomes. When healthcare practitioners perceive AI technology as adding extra workload, such techno-overload will undermine the importance of the justice signal and subsequently affect their attitudes, satisfaction, and usage intentions with AI technology.","AI","University of Sheffield"
"A systematic review of artificial intelligence impact assessments.","https://eprints.whiterose.ac.uk/197705/","Stahl, B.C.; Antoniou, J.; Bhalla, N.","February 2023","Artificial intelligence (AI) is producing highly beneficial impacts in many domains, from transport to healthcare, from energy distribution to marketing, but it also raises concerns about undesirable ethical and social consequences. AI impact assessments (AI-IAs) are a way of identifying positive and negative impacts early on to safeguard AI’s benefits and avoid its downsides. This article describes the first systematic review of these AI-IAs. Working with a population of 181 documents, the authors identified 38 actual AI-IAs and subjected them to a rigorous qualitative analysis with regard to their purpose, scope, organisational context, expected issues, timeframe, process and methods, transparency and challenges. The review demonstrates some convergence between AI-IAs. It also shows that the field is not yet at the point of full agreement on content, structure and implementation. The article suggests that AI-IAs are best understood as means to stimulate reflection and discussion concerning the social and ethical consequences of AI ecosystems. Based on the analysis of existing AI-IAs, the authors describe a baseline process of implementing AI-IAs that can be implemented by AI developers and vendors and that can be used as a critical yardstick by regulators and external observers to evaluate organisations’ approaches to AI.","AI","University of Sheffield"
"An interpretable deep learning model for time-series electronic health records: Case study of delirium prediction in critical care.","https://eprints.whiterose.ac.uk/203592/","Sheikhalishahi, S.; Bhattacharyya, A.; Celi, L.A.","September 2023","Deep Learning (DL) models have received increasing attention in the clinical setting, particularly in intensive care units (ICU). In this context, the interpretability of the outcomes estimated by the DL models is an essential step towards increasing adoption of DL models in clinical practice. To address this challenge, we propose an ante-hoc, interpretable neural network model. Our proposed model, named double self-attention architecture (DSA), uses two attention-based mechanisms, including self-attention and effective attention. It can capture the importance of input variables in general, as well as changes in importance along the time dimension for the outcome of interest. We evaluated our model using two real-world clinical datasets covering 22840 patients in predicting onset of delirium 12 h and 48 h in advance. Additionally, we compare the descriptive performance of our model with three post-hoc interpretable algorithms as well as with the opinion of clinicians based on the published literature and clinical experience. We find that our model covers the majority of the top-10 variables ranked by the other three post-hoc interpretable algorithms as well as the clinical opinion, with the advantage of taking into account both, the dependencies among variables as well as dependencies between varying time-steps. Finally, our results show that our model can improve descriptive performance without sacrificing predictive performance.","AI","University of Sheffield"
"The struggle for news value in the digital era.","https://eprints.whiterose.ac.uk/203244/","Harcup, T.","August 2023","News has long been a contested concept but in the digital era it has become increasingly fractured and multidimensional. This discursive article explores some of the ways in which the news has been disrupted by technological and economic tensions and argues that the social value of news is worth articulating and, where necessary, struggling for. News values have never been universal or unproblematic, and the tension between commercial and social ways of valuing news is intensified today. News values are not fixed and must be open to critique as to how they are meeting citizens’ needs. Societally useful news may be at risk of being marginalized as news organizations struggle to survive, but it is not inevitable that disruption and digitization should undermine journalistic ethics and the social value of news. In arguing that scholars ought to approach news more holistically, to defend it as well as critique it, the article attempts to synthesize what typically appears as discrete approaches to studying news. The article concludes that, if the social value of news is not to suffer further diminution, there is a need to view news through a lens of struggle; a struggle in which journalists, audiences, scholars and, indeed, all citizens have a part to play.","AI","University of Sheffield"
"Machine learning‐based predictions of buckling behaviour of cold‐formed steel structural elements.","https://eprints.whiterose.ac.uk/203497/","Mojtabaei, S.M.; Becque, J.; Khandan, R.","September 2023","Designing thin-walled structural members is a complex process due to the possibility of multiple instabilities. This study aimed to develop machine learning algorithms to predict the buckling behavior of thin-walled channel elements under axial compression or bending. The algorithms were trained using feed-forward multi-layer Artificial Neural Networks (ANNs), with the input variables including the cross-sectional dimensions, the thickness, the presence and location of intermediate stiffeners, and the element length. The output data included the elastic critical buckling load or moment, as well as a modal decomposition of the buckled shape into the pure buckling mode categories: local, distortional and global buckling. The Finite Strip Method (FSM) and the Equivalent Nodal Force Method (ENFM) were used to prepare the sample output for training. To ensure the accuracy of the developed algorithms, the ANN models were subjected to a K-fold cross-validation technique and featured optimized hyperparameters. The results showed that the trained algorithms had a remarkable accuracy of 98% in predicting the elastic critical buckling loads and modal decomposition of the critical buckled shapes.","AI","University of Sheffield"
"Artificial intelligence for diagnostic and prognostic neuroimaging in dementia: a systematic review.","https://eprints.whiterose.ac.uk/202425/","Borchert, R.J.; Azevedo, T.; Badhwar, A.","June 2023","Introduction Artificial intelligence (AI) and neuroimaging offer new opportunities for diagnosis and prognosis of dementia.","AI","University of Sheffield"
"“So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy.","https://eprints.whiterose.ac.uk/197691/","Dwivedi, Y.K.; Kshetri, N.; Hughes, L.","March 2023","Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.","AI","University of Sheffield"
"Exploring ethics and human rights in artificial intelligence – a Delphi study.","https://eprints.whiterose.ac.uk/197690/","Stahl, B.C.; Brooks, L.; Hatzakis, T.","March 2023","Ethical and human rights issues of artificial intelligence (AI) are a prominent topic of research and innovation policy as well as societal and scientific debate. It is broadly recognised that AI-related technologies have properties that can give rise to ethical and human rights concerns, such as privacy, bias and discrimination, safety and security, economic distribution, political participation or the changing nature of warfare. Numerous ways of addressing these issues have been suggested. In light of the complexity of this discussion, we undertook a Delphi study with experts in the field to determine the most pressing issues and prioritise appropriate mitigation strategies. The results of the study demonstrate the difficulty of defining clear priorities. Our findings suggest that the debate around ethics and human rights of AI would benefit from being reframed and more strongly emphasising the systems nature of AI ecosystems.","AI","University of Sheffield"
"Artificial intelligence–based ethical hacking for health information systems: simulation study.","https://eprints.whiterose.ac.uk/199262/","He, Y.; Zamani, E.; Yevseyeva, I.","January 2023","Background:","AI","University of Sheffield"
"Chatbots in libraries: a systematic literature review.","https://eprints.whiterose.ac.uk/204750/","Yan, R.; Zhao, X.; Mazumdar, S.","April 2023","Chatbots have experienced significant growth over the past decade, with a proliferation of new applications across various domains. Previous studies also demonstrate the trend of new technologies, especially artificial intelligence, being adopted in libraries. The purpose of this study is to determine the current research priorities and findings in the field of chatbots in libraries. A systematic literature review was performed utilising the PRISMA checklist and the databases Scopus and Web of Science, identifying 5734 records. Upon conducting the first screening, abstract screening, full-text assessment, and quality assessments guided by the CASP appraisal checklist, 19 papers were deemed suitable for inclusion in the review. The results of the review indicate that the majority of the existing studies were empirical in nature (primarily adopting qualitative methods) and technology reviews with a focus on reviewing the implementation and maintenance, design, evaluation, characteristics, and application of chatbots. The chatbots of interest were mainly text-based and guided chatbots, with closed-source tools with access portals mostly built on library web pages or integrated with social software. The research findings primarily concerned the development models and necessary tools and technologies, the application of chatbots in libraries. Our systematic review also suggests that studies on chatbots in libraries are still in the early stages.","AI","University of Sheffield"
"Are we nearly there yet? A desires & realities framework for Europe's AI strategy.","https://eprints.whiterose.ac.uk/185427/","Polyviou, A.; Zamani, E.D.","April 2022","Of all emerging technologies, Artificial Intelligence (AI) is perhaps the most debated topic in contemporary society because it promises to redefine and disrupt several sectors. At the same time, AI poses challenges for policymakers and decision-makers, particularly regarding formulating strategies and regulations to address their stakeholders’ needs and perceptions. This paper explores stakeholder perceptions as expressed through their participation in the formulation of Europe's AI strategy and sheds light on the challenges of AI in Europe and the expectations for the future. Our analysis reveals six dimensions towards an AI strategy; ecosystems, education, liability, data availability sufficiency & protection, governance and autonomy. It draws on these dimensions to construct a desires-realities framework for AI strategy in Europe and provide a research agenda for addressing existing realities. Our findings contribute to understanding stakeholder desires on AI and hold important implications for research, practice and policymaking.","AI","University of Sheffield"
"The Importance of mitral valve prolapse doming volume in the assessment of left ventricular stroke volume with cardiac MRI.","https://eprints.whiterose.ac.uk/196827/","Li, R.; Assadi, H.; Matthews, G.","January 2023","There remains a debate whether the ventricular volume within prolapsing mitral valve (MV) leaflets should be included in the left ventricular (LV) end-systolic volume, and therefore factored in LV stroke volume (SV), in cardiac magnetic resonance (CMR) assessments. This study aims to compare LV volumes during end-systolic phases, with and without the inclusion of the volume of blood on the left atrial aspect of the atrioventricular groove but still within the MV prolapsing leaflets, against the reference LV SV by four-dimensional flow (4DF). A total of 15 patients with MV prolapse (MVP) were retrospectively enrolled in this study. We compared LV SV with (LV SVMVP) and without (LV SVstandard) MVP left ventricular doming volume, using 4D flow (LV SV4DF) as the reference value. Significant differences were observed when comparing LV SVstandard and LV SVMVP (p < 0.001), and between LV SVstandard and LV SV4DF (p = 0.02). The Intraclass Correlation Coefficient (ICC) test demonstrated good repeatability between LV SVMVP and LV SV4DF (ICC = 0.86, p < 0.001) but only moderate repeatability between LV SVstandard and LV SV4DF (ICC = 0.75, p < 0.01). Calculating LV SV by including the MVP left ventricular doming volume has a higher consistency with LV SV derived from the 4DF assessment. In conclusion, LV SV short-axis cine assessment incorporating MVP dooming volume can significantly improve the precision of LV SV assessment compared to the reference 4DF method. Hence, in cases with bi-leaflet MVP, we recommend factoring in MVP dooming into the left ventricular end-systolic volume to improve the accuracy and precision of quantifying mitral regurgitation.","AI","University of Sheffield"
"Using the dual concept of evolutionary game and reinforcement learning in support of decision-making process of community regeneration—case study in Shanghai.","https://eprints.whiterose.ac.uk/196130/","Zhou, Y.; Lei, H.; Zhang, X.","January 2023","Under the digital revolution that spawned in recent years, AI support is raised in the context of urban design and governance as it aims to match the operation of the urban developing process. It offers more chances for ensuring equality in public participation and empowerment, with the possibility of projection and computation of integrated social, cultural, and physical spaces. Therefore, this research explored how scenario simulation of social attributes and social interaction dimensions can be incorporated into digital twin city research and development, which is seen as a problem to be addressed in the refinement and planning of future digital platforms and management in terms of decision-making. To achieve the research aim, this paper examined the evolution of social governance state and strain decision models, built a simulation method for the evolution of complex systems of social governance driven by the fusion of data and knowledge, and proposed a system response to residents’ ubiquitous perception and ubiquitous participation. The findings can help inspire the application of computational decision-making support in urban governance, and enhance the internal drive for comprehensive and sustainable urban regeneration. Moreover, they imply the role of the updated iterations of physical space and social interaction on social attributes.","AI","University of Sheffield"
"An evolving feature weighting framework for radial basis function neural network models.","https://eprints.whiterose.ac.uk/194047/","Muda, M.Z.; Solis, A.R.; Panoutsos, G.","November 2022","Via Granular Computing (GrC), one can create effective computational frameworks for obtaining information from data, motivated by the human perception of combining similar objects. Combining knowledge gained via GrC with a Fuzzy inference engine (Neural-Fuzzy) enable us to develop a transparent system. While weighting variables based on their importance during the iterative data granulation process has been proposed before (W-GrC), there is no work in the literature to demonstrate effectiveness and impact on Type-2 Fuzzy Logic systems (T2-FLS). The main contribution of this paper is to extend W-GrC, for the first time, to both Type-1 and Type-2 models known as Radial Basis Function Neural Network (RBFNN) and General Type-2 Radial Basis Function Neural Network (GT2-RBFNN). The proposed framework is validated using popular datasets: Iris, Wine, Breast Cancer, Heart and Cardiotocography. Results show that with the appropriate selection of feature weight parameter, the new computational framework achieves better classification accuracy outcomes. In addition, we also introduce in this research work an investigation on the modelling structure's interpretability (via Nauck's index) where it is shown that a good balance of interpretability and accuracy can be maintained.","AI","University of Sheffield"
"Applying Artificial Intelligence to wearable sensor data to diagnose and predict cardiovascular disease: a review.","https://eprints.whiterose.ac.uk/193055/","Huang, J.-D.; Wang, J.; Ramsey, E.","October 2022","Cardiovascular disease (CVD) is the world’s leading cause of mortality. There is significant interest in using Artificial Intelligence (AI) to analyse data from novel sensors such as wearables to provide an earlier and more accurate prediction and diagnosis of heart disease. Digital health technologies that fuse AI and sensing devices may help disease prevention and reduce the substantial morbidity and mortality caused by CVD worldwide. In this review, we identify and describe recent developments in the application of digital health for CVD, focusing on AI approaches for CVD detection, diagnosis, and prediction through AI models driven by data collected from wearables. We summarise the literature on the use of wearables and AI in cardiovascular disease diagnosis, followed by a detailed description of the dominant AI approaches applied for modelling and prediction using data acquired from sensors such as wearables. We discuss the AI algorithms and models and clinical applications and find that AI and machine-learning-based approaches are superior to traditional or conventional statistical methods for predicting cardiovascular events. However, further studies evaluating the applicability of such algorithms in the real world are needed. In addition, improvements in wearable device data accuracy and better management of their application are required. Lastly, we discuss the challenges that the introduction of such technologies into routine healthcare may face","AI","University of Sheffield"
"Guest editorial: artificial intelligence for B2B marketing: challenges and opportunities.","https://eprints.whiterose.ac.uk/188116/","Dwivedi, Y.K.; Wang, Y.","June 2022","A growing body of evidence indicates that implementing artificial intelligence (AI) at scale can improve market performance in B2B settings by accelerating decision-making process. Despite its popularity in the B2B sector, there have been few academic studies about this phenomenon in the context of industrial markets. Currently, AI research focuses predominantly on the marketing aspect of consumers, but in fact industrial data is rarely analyzed to address the issues regarding organizational behavior, product innovation, supply chain management, and B2B customer relationship management. The special issue presents 16 papers that explore why do B2B companies seek to use AI for marketing purposes, how AI can be used to foster innovation and use supply chain networks, how AI can enhance B2B customer experience and customer relationship management, and how AI can be used to develop dynamic capabilities on B2B marketing. These research articles provide insights into various industrial contexts and have applied both qualitative and quantitative approaches to identify AI applications for value creation.","AI","University of Sheffield"
"Video-based table tennis tracking and trajectory prediction using convolutional neural networks.","https://eprints.whiterose.ac.uk/194374/","Li, H.; Ali, S.G.; Zhang, J.","November 2021","One of the fascinating aspects of sports rivalry is that anything can happen. The significant difficulty is that computer-aided systems must address how to record and analyze many game events, and fractal AI plays an essential role in dealing with complex structures, allowing effective solutions. In table tennis, we primarily concentrate on two issues: ball tracking and trajectory prediction. Based on these two components, we can get ball parameters such as velocity and spin, perform data analysis, and even create a ping-pong robot application based on fractals. However, most existing systems rely on a traditional method based on physical analysis and a non-machine learning tracking algorithm, which can be complex and inflexible. As mentioned earlier, to overcome the problem, we proposed an automatic table tennis-aided system based on fractal AI that allows solving complex issues and high structural complexity of object tracking and trajectory prediction. For object tracking, our proposed algorithm is based on structured output Convolutional Neural Network (CNN) based on deep learning approaches and a trajectory prediction model based on Long Short-Term Memory (LSTM) and Mixture Density Networks (MDN). These models are intuitive and straightforward and can be optimized by training iteratively on a large amount of data. Moreover, we construct a table tennis auxiliary system based on these models currently in practice.","AI","University of Sheffield"
"Leveraging Artificial Intelligence (AI) technology for English writing: introducing Wordtune as a digital writing assistant for EFL writers.","https://eprints.whiterose.ac.uk/192748/","Zhao, X.","May 2022","Artificial intelligence (AI) technologies have contributed significantly to the advancement of society. In recent years, AI-powered writing assistants have received increasing attention among English as a Foreign Language (EFL) communities. However, most of these digital writing tools focus on the revision and editing stages. Few digital tools are developed to help users during the writing process, such as assisting users in formulating or translating their ideas into writing. Wordtune is an AI-powered writing assistant that understands the writer’s ideas and suggests options for rewriting them using different tones (e.g. casual, formal) and lengths (e.g. shorten, expand). This tool can help EFL writers maintain a continuous flow and learn useful ways to express their ideas in written English. This tech review aims to provide an overview of Wordtune and its affordance in English writing for EFL writers, while also addressing the benefits and limitations of this technology.","AI","University of Sheffield"
"SHIFTing artificial intelligence to be responsible in healthcare: a systematic review.","https://eprints.whiterose.ac.uk/183475/","Siala, H.; Wang, Y.","February 2022","A variety of ethical concerns about artificial intelligence (AI) implementation in healthcare have emerged as AI becomes increasingly applicable and technologically advanced. The last decade has witnessed significant endeavors in striking a balance between ethical considerations and health transformation led by AI. Despite a growing interest in AI ethics, implementing AI-related technologies and initiatives responsibly in healthcare settings remains a challenge. In response to this topical challenge, we reviewed 253 articles pertaining to AI ethics in healthcare published between 2000 and 2020, summarizing the coherent themes of responsible AI initiatives. A preferred reporting items for systematic review and meta-analysis (PRISMA) approach was employed to screen and select articles, and a hermeneutic approach was adopted to conduct systematic literature review. By synthesizing relevant knowledge from AI governance and ethics, we propose a responsible AI initiative framework that encompasses five core themes for AI solution developers, healthcare professionals, and policy makers. These themes are summarized in the acronym SHIFT: Sustainability, Human centeredness, Inclusiveness, Fairness, and Transparency. In addition, we unravel the key issues and challenges concerning responsible AI use in healthcare, and outline avenues for future research.","AI","University of Sheffield"
"A secure blockchain platform for supporting AI-enabled IoT applications at the edge layer.","https://eprints.whiterose.ac.uk/184868/","Alrubei, S.M.; Ball, E.; Rigelsford, J.M.","February 2022","In this study, a new blockchain protocol and a novel architecture that integrate the advantages offered by edge computing, artificial intelligence (AI), IoT end-devices, and blockchain were designed, developed, and validated. This new architecture has the ability to monitor the environment, collect data, analyze it, process it using an AI-expert engine, provide predictions and actionable outcomes, and finally share it on a public blockchain platform. For the use-case implementation, the pandemic caused by the wide and rapid spread of the novel coronavirus COVID-19 was used to test and evaluate the proposed system. Recently, various authors traced the spread of viruses in sewage water and studied how it can be used as a tracking system. Early warning notifications can allow governments and organizations to take appropriate actions at the earliest stages possible. The system was validated experimentally using 14 Raspberry Pis, and the results and analyses proved that the system is able to utilize low-cost and low-power flexible IoT hardware at the processing layer to detect COVID-19 and predict its spread using the AI engine, with an accuracy of 95%, and share the outcome over the blockchain platform. This is accomplished when the platform is secured by the honesty-based distributed proof of authority (HDPoA) and without any substantial impact on the devices’ power sources, as there was only a power consumption increase of 7% when the Raspberry Pi was used for blockchain mining and 14% when used to produce an AI prediction.","AI","University of Sheffield"
"The ethics of AI for information professionals: eight scenarios.","https://eprints.whiterose.ac.uk/187487/","Cox, A.","May 2022","Artificial Intelligence (AI) is central to transformative changes happening in many industries, perhaps potentially to a fourth industrial revolution, but it has also raised a storm of ethical concerns. Information professionals need to navigate these ethical issues effectively because they are likely to use AI in delivering services as well as contributing to the process of adoption of AI more widely in their organisations. Professional ethical codes are too high level to offer precise or complete guidance. In this context, the purpose of this paper is to review the relevant literature and describe eight ethics scenarios of AI which have been developed specifically for information professionals to understand the issues in a concrete form. The paper considers how AI might be defined and presents some of the applications relevant to the information profession. It then summarises the key ethical issues raised by AI in general both those inherent to the technology and those arising from the nature of the AI industry. It considers existing studies that have discussed aspects of the ethical issues specifically for information professionals. It then describes a set of eight ethics scenarios that have been developed and shared in an open form to promote their reuse.","AI","University of Sheffield"
"Cyber-physical components of an autonomous and scalable SLES.","https://eprints.whiterose.ac.uk/182934/","Verba, N.; Baldivieso-Monasterios, P.; Dong, S.","December 2021","Adding renewable energy sources and storage units to an electric grid has led to a change in the way energy is generated and billed. This shift cannot be managed without a unified view of energy systems and their components. This unified view is captured within the idea of a Smart Local Energy System (SLES). Currently, various isolated control and market elements are proposed to resolve network constraints, demand side response and utility optimisation. They rely on topology estimations, forecasting and fault detection methods to complete their tasks. This disjointed design has led to most systems being capable of fulfilling only a single role or being resistant to change and extensions in functionality. By allocating roles, functional responsibilities and technical requirements to bounded systems a more unified view of energy systems can be achieved. This is made possible by representing an energy system as a distributed peer-to-peer (P2P) environment where each individual demand energy resource (DER) on the consumer's side of the meter is responsible for their portion of the network and can facilitate trade with numerous entities including the grid. Advances in control engineering, markets and services such as forecasting, topology identification and cyber-security can enable such trading and communication to be done securely and robustly. To enable this advantage however, we need to redefine how we view the design of the sub-systems and interconnections within smart local energy systems (SLES). In this paper we describe a way in which whole system design could be achieved by integrating control, markets and analytics into each system. We propose the use of physical, control, market and service layers to create system of systems representation.","AI","University of Sheffield"
"Cultural proximity bias in AI-acceptability : the importance of being human.","https://eprints.whiterose.ac.uk/185247/","Tubadji, A.; Huang, H.; Webber, D.J.","August 2021","Artificial intelligence (AI) can generate a greater number of recombinations of ideas than humans can, and hence AI-produced creative products could be seen as embodying more innovation and surprise which are worth higher economic value. Yet the lack of human emotionality embedded in an AI product deprives it of an essential ‘humanness’ to which people attach important cultural value. As the overall value of a product is a sum of its economic and cultural values, we assessed the demand differential and quality perception asymmetry of creative products, specifically music compositions, that have been created by humans and AI separately. We conducted a survey with a quasi-experimental design and found that respondents reveal lower valuations towards music generated by AI and will moderate their evaluations of quality away from AI- and towards human-generated compositions when the type of composer is known. The demand for creative goods is sensitive to consumers’ perceptions of cultural proximity to humanness that determine the acceptability of AI products.","AI","University of Sheffield"
"Fuzzy multi-criteria decision-making: example of an explainable classification framework.","https://eprints.whiterose.ac.uk/182962/","Yusuf, H.; Yang, K.; Panoutsos, G.; Jansen, T.; Jensen, R.; Mac Parthaláin, N.; Lin, C.-M.","November 2021","Explanation, or system interpretability, has always been important in applications where critical decisions need to be made, for example in the justice system or biomedical applications. In artificial intelligence and machine learning, there is an ever increasing need for system interpretability. This paper investigates a Fuzzy Multi-Criteria Decision-Making (MCDM) model as the basis for an interpretable framework for explainable classification. The proposed framework includes a Fuzzy Inference System paired with a modified MCDM-based model for data-driven classification. The modular nature of MCDM allows for the development of a model-based layer capable of generating factual and counterfactual explanations. Results on a ‘Titanic’ survivors’ dataset classification, which illustrates a minimal trade-off in predictive performance while gaining textual and graphical explanation, autonomously provided by the proposed model-based MCDM framework.","AI","University of Sheffield"
"Smart Manufacturing.","https://eprints.whiterose.ac.uk/177670/","Clough, P.; Stammers, J.; Jain, S.; Murugesan, S.","November 2021","The ability to connect a growing range of technologies, such as sensors, Internet of (Industrial) Things, cloud computing, Big Data analytics, AI, mobile devices and Augmented/Virtual Reality, is helping to take manufacturing to new levels of ‘smartness’. Such technologies have the opportunity to transform, automate and bring intelligence to manufacturing processes and support the next manufacturing era. In this chapter, we describe the manufacturing context, emerging concepts, such as Industry 4.0, and technologies that are driving change and innovation within the manufacturing industry.","AI","University of Sheffield"
"Value co-creation in industrial AI: The interactive role of B2B supplier, customer and technology provider.","https://eprints.whiterose.ac.uk/192916/","Li, S.; Peng, G.; Xing, F.","July 2021","This research explores the interactive role of supplier, customer and technology company in business-to-business (B2B) marketing when they develop and use industrial artificial intelligence (AI). From a value co-creation perspective and following a service-dominant logic, this study aims to identify essential value types that are created collaboratively by B2B professionals (namely suppliers, customers and AI providers), and critical capabilities that contribute to their value co-creation practices. Nineteen in-depth semi-structured interviews were conducted with three groups of B2B stakeholders in six companies that involved in an industrial AI development and usage project. The data was then analysed using a thematic analysis approach. The results of this research contain a categorisation of four value types and three sets of capabilities, together with the interrelationships between them. This study contributes to the literature of value co-creation, information system and B2B marketing by bridging these three disciplines within the context of industrial AI development and usage.","AI","University of Sheffield"
"Active learning by acquiring contrastive examples.","https://eprints.whiterose.ac.uk/178426/","Margatina, K.; Vernikos, G.; Barrault, L.","September 2021","Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.","AI","University of Sheffield"
"Frustratingly simple pretraining alternatives to masked language modeling.","https://eprints.whiterose.ac.uk/178206/","Yamaguchi, A.; Chrysostomou, G.; Margatina, K.","September 2021","Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE's parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.","AI","University of Sheffield"
"An empirical study on leveraging position embeddings for target-oriented opinion words extraction.","https://eprints.whiterose.ac.uk/178208/","Mensah, S.; Sun, K.; Aletras, N.","September 2021","Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the target. However, the performance of these methods depends on the ability to incorporate this information into word representations. In this paper, we explore a variety of text encoders based on pretrained word embeddings or language models that leverage part-of-speech and position embeddings, aiming to examine the actual contribution of each component in TOWE. We also adapt a graph convolutional network (GCN) to enhance word representations by incorporating syntactic information. Our experimental results demonstrate that BiLSTM-based models can effectively encode position information into word representations while using a GCN only achieves marginal gains. Interestingly, our simple methods outperform several state-of-the-art complex neural structures.","AI","University of Sheffield"
"PyKale: knowledge-aware machine learning from multiple sources in Python.","https://eprints.whiterose.ac.uk/191957/","Lu, H.; Liu, X.; Turner, R.","June 2021","Machine learning is a general-purpose technology holding promises for many interdisciplinary research problems. However, significant barriers exist in crossing disciplinary boundaries when most machine learning tools are developed in different areas separately. We present Pykale - a Python library for knowledge-aware machine learning on graphs, images, texts, and videos to enable and accelerate interdisciplinary research. We formulate new green machine learning guidelines based on standard software engineering practices and propose a novel pipeline-based application programming interface (API). PyKale focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction, thus supporting multimodal learning and transfer learning (particularly domain adaptation) with latest deep learning and dimensionality reduction models. We build PyKale on PyTorch and leverage the rich PyTorch ecosystem. Our pipeline-based API design enforces standardization and minimalism, embracing green machine learning concepts via reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. We demonstrate its interdisciplinary nature via examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging.","AI","University of Sheffield"
"Highly efficient knowledge graph embedding learning with orthogonal procrustes analysis.","https://eprints.whiterose.ac.uk/173332/","Peng, X.; Chen, G.; Lin, C.","April 2021","Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.","AI","University of Sheffield"
"Variable instance-level explainability for text classification.","https://eprints.whiterose.ac.uk/173285/","Chrysostomou, G.; Aletras, N.","April 2021","Despite the high accuracy of pretrained transformer networks in text classification, a persisting issue is their significant complexity that makes them hard to interpret. Recent research has focused on developing feature scoring methods for identifying which parts of the input are most important for the model to make a particular prediction and use it as an explanation (i.e. rationale). A limitation of these approaches is that they assume that a particular feature scoring method should be used across all instances in a dataset using a predefined fixed length, which might not be optimal across all instances. To address this, we propose a method for extracting variable-length explanations using a set of different feature scoring methods at instance-level. Our method is inspired by word erasure approaches which assume that the most faithful rationale for a prediction should be the one with the highest divergence between the model's output distribution using the full text and the text after removing the rationale for a particular instance. Evaluation on four standard text classification datasets shows that our method consistently provides more faithful explanations compared to previous fixed-length and fixed-feature scoring methods for rationale extraction.","AI","University of Sheffield"
"Cross-lingual word embedding refinement by ℓ1 norm optimisation.","https://eprints.whiterose.ac.uk/173333/","Peng, X.; Lin, C.; Stevenson, M.","April 2021","Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the ℓ2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. ℓ1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the ℓ1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.","AI","University of Sheffield"
"Exploring the impact of Artificial Intelligence and robots on higher education through literature-based design fictions.","https://eprints.whiterose.ac.uk/170026/","Cox, A.M.","November 2020","Artificial Intelligence (AI) and robotics are likely to have a significant long-term impact on higher education (HE). The scope of this impact is hard to grasp partly because the literature is siloed, as well as the changing meaning of the concepts themselves. But developments are surrounded by controversies in terms of what is technically possible, what is practical to implement and what is desirable, pedagogically or for the good of society. Design fictions that vividly imagine future scenarios of AI or robotics in use offer a means both to explain and query the technological possibilities. The paper describes the use of a wide-ranging narrative literature review to develop eight such design fictions that capture the range of potential use of AI and robots in learning, administration and research. They prompt wider discussion by instantiating such issues as how they might enable teaching of high order skills or change staff roles, as well as exploring the impact on human agency and the nature of datafication.","AI","University of Sheffield"
"A secure distributed blockchain platform for use in AI-enabled IoT applications.","https://eprints.whiterose.ac.uk/167100/","Alrubei, S.; Ball, E.; Rigelsford, J.","October 2020","The increased implementation of Edge Computing technology has provided The Internet of Things (IoT) with the ability of real-time data processing and tasks execution requested by smart devices. To support this processing the integration of Artificial Intelligence (AI) into IoT is considered one of the most promising approach. While AI helps in the analyses of the data, blockchain technology provides a robust environment within which to create a secure, distributed way to share and store data. This paper proposes an architecture that combines the strengths provided by edge computing, AI, and blockchain technologies to provide robust, secure, and intelligent solutions for secure and faster data processing and sharing. The pandemic created by the rapid spread of the novel Coronavirus COVID19, as well as the tracking of viruses in water sewage to help control the spread of such viruses, were used as our case study for exploring this architecture. To secure the proposed architecture a new concept for consensus mechanism based on Honesty-Based Distributed Proof of Work (DPOW) were devised and tested.","AI","University of Sheffield"
"Explainable artificial intelligence for developing smart cities solutions.","https://eprints.whiterose.ac.uk/168306/","Thakker, D.; Mishra, B.K.; Abdullatif, A.","October 2020","Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions, Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative training datasets and features engineering and less on the available domain expertise. We argue that such an approach to solution development makes the outcome of solutions less explainable, i.e., it is often not possible to explain the results of the model. There is a growing concern among policymakers in cities with this lack of explainability of AI solutions, and this is considered a major hindrance in the wider acceptability and trust in such AI-based solutions. In this work, we survey the concept of ‘explainable deep learning’ as a subset of the ‘explainable AI’ problem and propose a new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring application in the context of a European Commission-funded project. Monitoring of gullies and drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc., and building a DL-based classifier to detect such objects and classify blockages based on the presence and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier, the DL component detects object presence and coverage level and semantic rules designed with close consultation with experts carry out the classification. By using the expert knowledge in the flooding context, our hybrid classifier provides the flexibility on categorising the image using objects and their coverage relationships. The experimental results demonstrated with a real-world use case showed that this hybrid approach of image classification has on average 11% improvement (F-Measure) in image classification performance compared to DL-only classifier. It also has the distinct advantage of integrating experts’ knowledge on defining the decision-making rules to represent the complex circumstances and using such knowledge to explain the results.","AI","University of Sheffield"
"Deep learning with electronic health records for short-term fracture risk identification : crystal bone algorithm development and validation.","https://eprints.whiterose.ac.uk/167712/","Almog, Y.A.; Rai, A.; Zhang, P.","September 2020","Background: Fractures as a result of osteoporosis and low bone mass are common and give rise to significant clinical, personal, and economic burden. Even after a fracture occurs, high fracture risk remains widely underdiagnosed and undertreated. Common fracture risk assessment tools utilize a subset of clinical risk factors for prediction, and often require manual data entry. Furthermore, these tools predict risk over the long term and do not explicitly provide short-term risk estimates necessary to identify patients likely to experience a fracture in the next 1-2 years.","AI","University of Sheffield"
"Repeatable determinism using non-random weight initialisations in smart city applications of deep learning.","https://eprints.whiterose.ac.uk/156191/","Rudd-Orthner, R.N.M.; Mihaylova, L.","December 2019","Modern Smart City applications draw on the need for requirements that are safe, reliable and sustainable, as such these applications have a need to utilise machine-learning mechanisms such that they are consistent with public liability. Machine and deep learning networks, therefore, are required to be in a form that is safe and deterministic in their development and also in their deployment. The viability of non-random weight initialisation schemes in neural networks make the network more deterministic in learning sessions which is a desirable property in safety critical systems where deep learning is applied to smart city applications and where public liability is a concern. The paper uses a variety of schemes over number ranges and gradients and achieved a 98.09% accuracy figure, + 0.126% higher than the original random number scheme at 97.964%. The paper highlights that in this case, it is the number range and not the gradient that is affecting the achieved accuracy most dominantly, although there can be a coupling of number range with activation functions used. Unexpectedly in this paper, an effect of numerical instability was discovered from run to run when run on a multi-core CPU. The paper also has shown the enforcement of consistent deterministic results on an multi-core CPU by defining atomic critical code regions, and that aids repeatable information assurance in model fitting (or learning sessions). That enforcement of consistent repeatable determinism has also a benefit to accuracy even for the random schemes, and a highest score of 98.29%, + 0.326% higher than the baseline was achieved. However, also the non-random initialisation scheme causes weight arrangements after learning to be more structured which has benefits for validation in safety critical applications.","AI","University of Sheffield"
"Toward an understanding of responsible artificial intelligence practices.","https://eprints.whiterose.ac.uk/162719/","Wang, Y.; Xiong, M.; Olya, H.; Bui, T.X.","January 2020","	Artificial Intelligence (AI) is influencing all aspects of human and business activities nowadays. Although potential benefits emerged from AI technologies have been widely discussed in many current literature, there is an urgently need to understand how AI can be designed to operate responsibly and act in a manner meeting stakeholders’ expectations and applicable regulations. We seek to fill the gap by exploring the practices of responsible AI and identifying the potential benefits when implementing responsible AI practices. In this study, 10 responsible AI cases were selected from different industries to better understand the use of responsible AI in practices. Four responsible AI practices are identified, including governance, ethically design solutions, risk control and training and education and five strategies for firms who are considering to adopt responsible AI practices are recommended.","AI","University of Sheffield"
